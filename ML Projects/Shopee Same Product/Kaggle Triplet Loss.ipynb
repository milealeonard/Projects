{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import tensorflow as tf\n",
    "import keras as k\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import np_utils\n",
    "import csv\n",
    "import os\n",
    "import math\n",
    "from random import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create pairs based on csv file\n",
    "csvfile = open('data/train.csv', 'r')\n",
    "csvreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "\n",
    "pairs = []\n",
    "prev_ids = []\n",
    "for i, row in enumerate(csvreader):\n",
    "    if i != 0:\n",
    "        curr_id = row[-1]\n",
    "        curr_name = row[1]\n",
    "\n",
    "        if curr_id in prev_ids:\n",
    "            index_of_pair = prev_ids.index(curr_id)\n",
    "            pairs[index_of_pair].append(curr_name)\n",
    "        else:\n",
    "            prev_ids.append(curr_id)\n",
    "            pairs.append([curr_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert filenames to np.arrays in the pairs\n",
    "for i in range(len(pairs)):\n",
    "    for j in range(len(pairs[i])):\n",
    "        y = plt.imread(os.path.join(\"data/train_images\", pairs[i][j]))/255\n",
    "        # y = np.asarray(y)/255\n",
    "        data = [y]\n",
    "        data = np.asarray(data)\n",
    "        pairs[i][j] = tf.image.resize(data, (80,80)).numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that pairs are chosen correctly\n",
    "nrows = 2\n",
    "ncols = 3\n",
    "fig, axes = plt.subplots(nrows, ncols)\n",
    "i = 0\n",
    "while i < nrows*ncols/2:\n",
    "    axes.flat[i].imshow(pairs[i][0])\n",
    "    axes.flat[i+ncols].imshow(pairs[i][1])\n",
    "    axes.flat[i].set_axis_off()\n",
    "    axes.flat[i+ncols].set_axis_off()\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to randomly generate a negative for the A, P, N Triplet\n",
    "def generate_random(not_this_pair):\n",
    "    rndm_class = math.floor(random()*len(pairs)) #choose random number up to len(pairs)\n",
    "    rndm_item = math.floor(random()*len(pairs[rndm_class]))\n",
    "    for arr in not_this_pair:\n",
    "        if np.array_equal(arr, rndm_item):\n",
    "            return generate_random(not_this_pair)\n",
    "        else:\n",
    "            return rndm_class, rndm_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort training data into triplets (anchor, positive, negative)\n",
    "x_train = []\n",
    "for pair in pairs:\n",
    "    for i in range(1, len(pair)):\n",
    "        temp = []\n",
    "        temp.append(pair[0])#anchor\n",
    "        temp.append(pair[i])#positive\n",
    "        #randomly choose image from database\n",
    "        rndm_class, rndm_item = generate_random(pair)\n",
    "        temp.append(pairs[rndm_class][rndm_item])\n",
    "        x_train.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load or save the training data for future use\n",
    "\n",
    "# np.save(\"x_train_array\", x_train)\n",
    "x_train = np.load(\"x_train_array.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training data into the three inputs for model (anchor, positive, negative)\n",
    "a_train = []\n",
    "p_train = []\n",
    "n_train = []\n",
    "for i in range(len(x_train)):\n",
    "    a_train.append(x_train[i][0])\n",
    "    p_train.append(x_train[i][1])\n",
    "    n_train.append(x_train[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that triplets are chosen correctly\n",
    "nrows = 3\n",
    "ncols = 3\n",
    "fig, axes = plt.subplots(nrows, ncols)\n",
    "img_number = 8\n",
    "for i in range(ncols):\n",
    "    axes.flat[i].imshow(x_train[img_number][i])\n",
    "    axes.flat[i+ncols].imshow(x_train[img_number+1][i])\n",
    "    axes.flat[i+2*ncols].imshow(x_train[img_number+2][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define our triplet loss function\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    anchor_out = y_pred[:, 0:100]\n",
    "    positive_out = y_pred[:, 100:200]\n",
    "    negative_out = y_pred[:, 200:300]\n",
    "\n",
    "    pos_dist = K.sum(K.abs(anchor_out-positive_out), axis=1)\n",
    "    neg_dist = K.sum(K.abs(anchor_out-negative_out), axis=1)\n",
    "\n",
    "    probs = K.softmax([pos_dist, neg_dist], axis=0)\n",
    "    return K.mean(K.abs(probs[0]) + K.abs(1.0-probs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create base 1 input model\n",
    "input_layer = Input((80, 80, 3))\n",
    "x = Conv2D(32, 3, activation=\"relu\")(input_layer)\n",
    "x = Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = MaxPooling2D(2)(x)\n",
    "x = Conv2D(64, 3, activation=\"relu\")(x)\n",
    "x = Conv2D(64, 3, activation=\"relu\")(x)\n",
    "x = MaxPooling2D(2)(x)\n",
    "x = Conv2D(128, 3, activation=\"relu\")(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(100)(x)\n",
    "model = Model(input_layer, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the concatenated model with 3 inputs\n",
    "\n",
    "triplet_model_a = Input((80, 80, 3))\n",
    "triplet_model_p = Input((80, 80, 3))\n",
    "triplet_model_n = Input((80, 80, 3))\n",
    "triplet_model_out = Concatenate()([model(triplet_model_a), model(triplet_model_p), model(triplet_model_n)])\n",
    "triplet_model = Model([triplet_model_a, triplet_model_p, triplet_model_n], triplet_model_out)\n",
    "triplet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile model with triplet_loss and adam optimiser\n",
    "\n",
    "triplet_model.compile(loss=triplet_loss, optimizer = \"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create data_generator which feeds in the triplets to model\n",
    "def data_generator(batch_size=64):\n",
    "    while True:\n",
    "        a = []\n",
    "        p = []\n",
    "        n = []\n",
    "        for i in range(batch_size):\n",
    "            a.append(a_train[i])\n",
    "            p.append(p_train[i])\n",
    "            n.append(n_train[i])\n",
    "        yield ([np.array(a), np.array(p), np.array(n)], np.zeros((batch_size, 1)).astype(\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train data\n",
    "triplet_model.fit(data_generator(), steps_per_epoch = 200, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to deduce whether two products (feeds in two np arrays) are the same using model\n",
    "def same_product(img_arr_one, img_arr_two):\n",
    "    testing = np.asarray([img_arr_one, img_arr_two])\n",
    "    predictions = triplet_model.layers[3].predict(testing)\n",
    "    distf = np.linalg.norm(predictions[0]-predictions[1])\n",
    "    if distf < 26.6: #arbitrary value selected to decide whether two products are similar enough\n",
    "        return [True, distf] \n",
    "    else:\n",
    "        return [False, distf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check accuracy (only on training_data because kaggle does not provide testing_data)\n",
    "errors = 0\n",
    "for triplet in x_train:\n",
    "    if same_product(triplet[0], triplet[1])[0] == False:\n",
    "        errors += 1\n",
    "    if same_product(triplet[0], triplet[2])[0] == True:\n",
    "        errors += 1\n",
    "    if same_product(triplet[2], triplet[1])[0] == True:\n",
    "        errors += 1\n",
    "print((1-errors/(3*len(x_train)))*100)\n",
    "#73.323453245436756% with 200 steps per epoch and size=(80, 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
